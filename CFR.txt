Due to the constraints of solving imperfect information games with MCTS and the memory limits of solving games with linear programs, CFR was developed as a novel solution. CFR also benefits from being computationally cheap and doesn't require parameter tuning. It is an iterative Nash equilibrium approximation method that works through the process of repeated self-play between two regret minimizing agents.

CFR is an extension of regret minimization into sequential games, where players play a sequence of actions to reach a terminal game state. Instead of storing and minimizing regret for the exponential number of strategies, CFR stores and minimizes a regret for each information set and its actions, which can be used to form an upper bound on the regret for any deterministic strategy. This means that we must also consider the probabilities of reaching each information set given the players' strategies, as well as passing forward game state information and probabilities of player actions, and passing backward utility information through the game information states. The algorithm stores a strategy and regret value for each action at each node, such that the space requirement is on the order O(|I|), where |I| is the number of information sets in the game.

CFR is an offline self-play algorithm, as it learns to play by repeatedly playing against itself. It begins with a strategy that is completely uniformly random and adjusts the strategy each iteration using regret matching such that the strategy at each node is proportional to the regrets for each action. The regrets are, as explained previously, measures of how the current strategy would have performed compared to a fixed strategy of always taking one particular action. Positive regret means that we would have done better by taking that action more often and negative regret means that we would have done better by not taking that action at all. The average strategy is then shown to approach a Nash equilibrium in the long run.

In the vanilla CFR algorithm, each iteration involves passing through every node in the extensive form of the game. Each pass evaluates strategies for both players by using regret matching, based on the prior cumulative regrets at each player's information sets. Before looking at the CFR equations, we will refresh some definitions that were given in previous sections here when they are relevant to the forthcoming equations.

Let A denote the set of all game actions. We refer to a strategy profile that excludes player i's strategy as σ(-i). A history h is a sequence of actions, including chance outcomes, starting from the root of the game. Let p_i(σ)(h) be the reach probability of game history h with strategy profile σ and p_i^σ(h,z) be the reach probability that begins at h and ends at z.

Let Z denote the set of all terminal game histories and then we have h⊏z for z∈Z is a nonterminal game history. Let u_i(z) denote the utility to player i of terminal history z.

We can now define the counterfactual value at nonterminal history h as follows: 
v_i(σ,h) = ∑(z∈Z, h⊏z) p_i^σ(-i) * p_i^σ(h,z) * u_i(z)

This is the expected utility to player i of reaching nonterminal history h and taking action a under the counterfactual assumption that player i takes actions to do so, but otherwise player i and all other players follow the strategy profile sigma.

The counterfactual value takes a player's strategy and history and returns a value that is the product of the reach probability of the opponent (and chance) to arrive to that history and the expected value of the player for all possible terminal histories from that point. This is counterfactual because we ignore the probabilities that factually came into player i's play to reach position h, which means that he is not biasing his future strategy with his current strategy. This weights the regrets by how often nature (factors outside the player's control, including chance and opponents) reach this information state. This is intuitive because states that are more frequently played to by opponents are more important to play profitably.

An information set is a group of histories that a player cannot distinguish between. Let I denote an information set and let A(I) denote the set of legal actions for information set I. Let σ(I-->a) denote a profile equivalent to sigma, except that action a is always chosen at information set I. The counterfactual regret of not taking action a at history h is defined as:

r(h,a) = v_i(σ(i-->a),h) - v_i(σ,h)

This is the difference between the value when always selecting action a at the history node and the value of the history node itself (which will be defined in more detail shortly).

Let p_i^σ(I) be the probability of reaching information set I through all possible game histories in I. Therefore we have that p_i^σ(I) = ∑(h∈I) p_i^σ(h). The counterfactual reach probability of information state I, p^σ(-i)(I), is the probability of reaching I with strategy profile σ except that, we treat current player I actions to reach the state as having probability 1.

The counterfactual regret of not taking action a at information set I is: 
r(I,a) = ∑(h∈I) r(h,a)

This calculation simply includes all histories in the information set.

Let t and T denote time steps, where t is with respect to each fixed information set and is incremented with each visit to an information set. A strategy σ^t_i for player i maps each player i information set I_i and legal player i action a∈A(I_i) to the probability that the player will choose a in I_i at time t. All player strategies together at time t form a strategy profile σ^t, to be detailed shortly.

If we define r^t_i(I,a) as the regret when players use σ^t of not taking action a at information set I belonging to player i, then we can define the cumulative counterfactual regret as follows, which is the summation over all time steps:

R^T_i(I,a) = ∑(t=1 to T) r^t_i(I,a)

In recent years, researchers have redefined the counterfactual value in terms of information sets. This formulation shows the counterfactual value for a particular information set and action, given a player and his strategy:

v^σ_i(I,a) = ∑(h∈I) ∑(z∈Z: h⊏z) u_i(z) p_i^σ(-i)(z) p_i^σ:I-->a_i(h,z)

We see that this is similar to the first equation for the counterfactual value, but has some differences. Because we are now calculating the value for an information set, we must sum over all of the relevant histories. The inner summation adds all possible leaf nodes that can be reached from the current history (same as the original one) and the outer summation adds all histories that are part of the current information set.

From left to right, the three terms on the right hand side represent the main player's utility at the leaf node z, the opponent and chance combined reach probability for the leaf node z, and the reach probability of the main player to go from the current history to the leaf node z, while always taking action a. The differences between this formulation and that of the original equation will be reconciled with the next equation.

The counterfactual regret of player i for action a at information set I can be written as follows:

R^T_i(I,a) = ∑(t=1,T) v^σt_i(I,a) - ∑(t=1,T) ∑(a'∈A) v^σT_i(I,a') σ^t_i(a'|I)

This formulation combines the three equations, where one had introduced the cumulative summation, one added all histories in the information set, and one defined the counterfactual regret difference equation. The first part of the difference in the counterfactual regret equation computes this value for the given a value, while the second part computes the expected value of all other a value options at the information set.

The inner summation of this part of the equation is over all non-a strategies and the outer summation is over all times. The first term in the summations computes the counterfactual value for each non-a strategy and the second term multiplies the counterfactual value by the player's probability of playing that particular strategy at the given information set.

We can show the regret-matching algorithm by first defining the nonnegative counterfactual regret as R^T,+_i(I,a) = max(R^T_i(I,a),0). Now we can use the cumulative regrets to obtain the strategy decision for the next iteration using regret matching:

Case 1 when ∑(a'∈A) R^(t-1)_i(I,a'))^+ > 0 then
σ^t_i(a|I) = (R^(t-1)_i(i,a))^+ / (∑(a'∈A) R^(t-1)_i(I,a'))^+)

Case 2 otherwise then
σ^t_i(a|I) = 1/|A|

This regret matching formula calculates the action probabilities for each action at each information set in proportion to the positive cumulative regrets. First we check to see if the cumulative regrets at the previous time step are positive. If not, the strategy is set to be uniformly random, determined by the number of available actions. If it is, then the strategy is the ratio of the cumulative regret of the defined action over the sum of the cumulative regrets of all other actions.

The CFR algorithm works by taking these action probabilities and then producing the next state in the game and computing the utilities of each action recursively. Regrets are computed from the returned values and the value of playing to the current node is then computed and returned. Regrets are updated at the end of each iteration.